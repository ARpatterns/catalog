# Event-Condition-Action Pattern
To design reactive systems, breaking down the system’s behavior into discrete [events](#events-categories-in-ar-applications), [conditions](#condition-evaluation-within-spatial-ar-context), and [actions](#actions-in-ar-applications) provides a structured and modular approach. An event is a signal that something has occurred, such as the start of an AR session (`on:start`), a user tapping on an item (`on:tap`), or the detection of an image marker (`on:detect`). The no-code editors [Apple Reality Composer](https://developer.apple.com/augmented-reality/tools/) and [Adobe Aero](https://www.adobe.com/products/aero.html) are using a trigger-action mechanism to define the behavior of an AR scenario. We propose to use more flexible Event-Condition-Action (ECA) rules that perform an action in response to an event, provided that certain conditions are met. ECA rules are widely used in event-driven and reactive systems, such as active databases and workflow systems. In the context of AR patterns, ECA rules provide a generic abstraction of the reactive behavior of AR software systems.

## Event Categories in AR Applications
In software systems, events are typically generated by a producer and triggered by various circumstances. These events can vary greatly in nature. Regarding AR systems, we organized typical events into distinct categories. Compared to other interactive 3D applications, the event categories [detection events](events.md#detection-events) and [data-driven events](events.md#data-driven-events) tend to dominate in AR.

| Event Category | Event Producer | Cause → Event Examples |
|---|---|---|
| [Session Event](events.md#session-events) | AR Session | State change → `on:start, on:locating` |
| [Invocation Event](events.md#invocation-events) | Rule Initiation | Invocation → `on:command, on:call` |
| [Detection Event](events.md#detection-events) | Installed Detector | Discovery of entity → `on:detect` |
| [User Event](events.md#user-events) | App User | User interaction → `on:tap, on:select` |
| [Temporal Event](events.md#temporal-events) | Time Scheduler | Elapsed time reached → `in:time` |
| [Data-driven Event](events.md#data-driven-events) | Data Observer | Value change → `on:altered, as:steady` | 
| [Response Event](events.md#response-events) | Remote Request | Response of REST call → `on:response` | 
| [Notification Event](events.md#notification-events) | Subscribed System | System change → `on:enter, on:leave` |

## Condition Evaluation Within Spatial AR Context
The data model reflecting the AR context is exposed for condition evaluation for any ECA rule. This model typically includes the following types of data:

| Data Type | Description |
|---|---|
| Session Data| time, date, device data, temporary data variables, UI mode |
| Location Data | longitude, latitude, address, country, ambience |
| User Data | position, orientation (tilt, yaw), name, user settings |
| Detected Occurrences | results of installed detectors
| Augmentation Items | model elements with visual representation (scene nodes) |

Conditions are typically formulated by predicates as logical statements using an expression syntax that has access to key-values in the data model. Some condition samples:
- `location.city == 'New York'`
- `location.environment == 'indoor'`
- `device.type == 'Phone'`
- `user.usesSpeech == true`
- `time.hour < 5`
- `data.var1 > 5.0`
- `walls.@count == 1`
- `'chair' IN detected.labels AND detected.confidence['chair'] > 0.8`

Additionally spatial functions can be included in condition evaluations such as:

| Check | Spatial Function | Description |
|---|---|---|
| Existence | `exist(ID) == 1` | does item with ID exist in AR world |
| Visibility | `visible(ID) == 1` | is item with ID visible to user |
| Proximity | `proximity(ID) <= 1.5` | distance in meters from user (virtual camera) to item with ID |
| Gazing | `gazingAt(ID) == 1` | is user gazing at item with ID |
| Geo-Distance | `distance(_lat, _long) < 50.0` | distance in meters from user to place in latitude/longitude |

## Actions in AR Applications
Actions in AR applications are mainly concerned with manipulating augmentation items and their visual representation, or with handling user activities (GUI reactions, audible feedback). Actions are expressed as `do:something` statements.

Staging of augmentation items is based on the content composition principle and executed by `do:add` actions. It involves assigning unique identifiers to visual and audible items and positioning them within the observed world relative to anchors of detected entities.

Common Actions in AR applications listed by category:

* [Item-Related Actions](actions.md#item-related-actions)
* [Visual-Related Actions](actions.md#visual-related-actions)
* [UI-Related Actions](actions.md#ui-related-actions)
* [Data-Related Actions](actions.md#data-related-actions)
* [Process-Related Actions](actions.md#process-related-actions)
* [Detector-Related Actions](actions.md#detector-related-actions)